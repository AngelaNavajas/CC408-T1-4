{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "370ed31d-cfba-4ea8-a3ae-ad372d2c3527",
   "metadata": {},
   "source": [
    "##  Parte I: Análisis de la base de hogares y tipo de ocupación\n",
    " Ahora que ya están familiarizados con la Encuesta Permanente de Hogares\n",
    " (EPH) y la desocupación, vamos a complejizar un poco la construcción de las\n",
    " tasas del desempleo. Relacionaremos la información a nivel hogar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf06b97-1597-4025-9c33-6fdd0eff08c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os  \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt  \n",
    "import statsmodels.api as sm     \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bfe7f4-7a76-4ece-ab06-4c32302ac5a6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Bases Tizi\n",
    "#base_ind_04_sucia = pd.read_stata(r\"C:\\Users\\tizip\\OneDrive\\Documentos\\Tizi UdeSA\\8- Ciencia de datos\\Compu nueva\\CC408-T1-4\\TP3\\Individual_t104.dta\")\n",
    "#base_ind_24_sucia = pd.read_excel(r\"C:\\Users\\tizip\\OneDrive\\Documentos\\Tizi UdeSA\\8- Ciencia de datos\\Compu nueva\\CC408-T1-4\\TP3\\usu_individual_T124.xlsx\")\n",
    "\n",
    "#base_hog_04_sucia = pd.read_stata(r\"C:\\Users\\tizip\\OneDrive\\Documentos\\Tizi UdeSA\\8- Ciencia de datos\\Compu nueva\\CC408-T1-4\\TP4\\Hogar_t104.dta\")\n",
    "#base_hog_24_sucia = pd.read_excel(r\"C:\\Users\\tizip\\OneDrive\\Documentos\\Tizi UdeSA\\8- Ciencia de datos\\Compu nueva\\CC408-T1-4\\TP4\\usu_hogar_T124.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e2c7a4ec-2882-4b4c-815b-54ebb0635766",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Bases Angie\n",
    "base_ind_04_sucia = pd.read_stata(r'/Users/angelanavajas/Desktop/Ciencia de datos/CC408-T1-4/TP3/Individual_t104.dta')\n",
    "base_ind_24_sucia = pd.read_excel(r'/Users/angelanavajas/Desktop/Ciencia de datos/CC408-T1-4/TP3/usu_individual_T124.xlsx')\n",
    "\n",
    "base_hog_04_sucia = pd.read_stata(r'/Users/angelanavajas/Desktop/Ciencia de datos/CC408-T1-4/TP4/Hogar_t104.dta')\n",
    "base_hog_24_sucia = pd.read_excel(r'/Users/angelanavajas/Desktop/Ciencia de datos/CC408-T1-4/TP4/usu_hogar_T124.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e3d604-56cb-463a-9bac-36b8674cc2e1",
   "metadata": {},
   "source": [
    "### PUNTO 1\n",
    "Exploren el diseño de registro de la base de hogar: a priori, ¿qué variables creen pueden ser predictivas de la desocupación y seria útil incluir para perfeccionar el ejercicio del TP3? Mencionen estas variables y justifiquen su elección."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908092ec-4393-45f9-8a75-51f1b6b3b377",
   "metadata": {},
   "source": [
    "### PUNTO 2\n",
    "Descarguen la base de microdatos de la EPH correspondiente al primer trimestre de 2004 y 2024 en formato .dta y .xls, respectivamente. La base de hogares se llama Hogar_t104.dta y usu_hogar_T124.xls, respectivamente. Eliminen todas las observaciones que no corresponden a los aglomerados de Ciudad Autónoma de Buenos Aires o Gran Buenos Aires y unan ambos trimestres en una sola base. Esto es, a la base de la encuesta individual de cada año (que usaron en el TP3) unan la base de la encuesta de hogar. Asegúrese de estar usando las variables CODUSU y NRO_Hogar para el merge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "87c3e870-92ab-4a0b-a74b-41a9bd70fe94",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codusu', 'ano4', 'trimestre', 'nro_hogar', 'componente', 'h15', 'region', 'mas_500', 'aglomerado', 'pondera', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch15_cod', 'ch16', 'ch16_cod', 'nivel_ed', 'estado', 'cat_ocup', 'cat_inac', 'imputa', 'pp02c1', 'pp02c2', 'pp02c3', 'pp02c4', 'pp02c5', 'pp02c6', 'pp02c7', 'pp02c8', 'pp02e', 'pp02h', 'pp02i', 'pp03c', 'pp03d', 'pp3e_tot', 'pp3f_tot', 'pp03g', 'pp03h', 'pp03i', 'pp03j', 'intensi', 'pp04a', 'pp04b_cod', 'pp04b1', 'pp04b2', 'pp04b3_mes', 'pp04b3_ano', 'pp04b3_dia', 'pp04c', 'pp04c99', 'pp04d_cod', 'pp04g', 'pp05b2_mes', 'pp05b2_ano', 'pp05b2_dia', 'pp05c_1', 'pp05c_2', 'pp05c_3', 'pp05e', 'pp05f', 'pp05h', 'pp06a', 'pp06c', 'pp06d', 'pp06e', 'pp06h', 'pp07a', 'pp07c', 'pp07d', 'pp07e', 'pp07f1', 'pp07f2', 'pp07f3', 'pp07f4', 'pp07f5', 'pp07g1', 'pp07g2', 'pp07g3', 'pp07g4', 'pp07g_59', 'pp07h', 'pp07i', 'pp07j', 'pp07k', 'pp08d1', 'pp08d4', 'pp08f1', 'pp08f2', 'pp08j1', 'pp08j2', 'pp08j3', 'pp09a', 'pp09a_esp', 'pp09b', 'pp09c', 'pp09c_esp', 'pp10a', 'pp10c', 'pp10d', 'pp10e', 'pp11a', 'pp11b_cod', 'pp11b1', 'pp11b2_mes', 'pp11b2_ano', 'pp11b2_dia', 'pp11c', 'pp11c99', 'pp11d_cod', 'pp11g_ano', 'pp11g_mes', 'pp11g_dia', 'pp11l', 'pp11l1', 'pp11m', 'pp11n', 'pp11o', 'pp11p', 'pp11q', 'pp11r', 'pp11s', 'pp11t', 'p21', 'decocur', 'idecocur', 'rdecocur', 'gdecocur', 'pdecocur', 'adecocur', 'pondiio', 'tot_p12', 'p47t', 'decindr', 'idecindr', 'rdecindr', 'gdecindr', 'pdecindr', 'adecindr', 'pondii', 'v2_m', 'v3_m', 'v4_m', 'v5_m', 'v8_m', 'v9_m', 'v10_m', 'v11_m', 'v12_m', 'v18_m', 'v19_am', 'v21_m', 't_vi', 'itf', 'decifr', 'idecifr', 'rdecifr', 'gdecifr', 'pdecifr', 'adecifr', 'ipcf', 'deccfr', 'ideccfr', 'rdeccfr', 'gdeccfr', 'pdeccfr', 'adeccfr', 'pondih', 'pj1_1', 'pj2_1', 'pj3_1', 'idimpp']\n"
     ]
    }
   ],
   "source": [
    "# LIMPIO BASE INDIVIDUAL\n",
    "\n",
    "# me quedo solo con los valores de CABA y GBA\n",
    "base_ind_04_filtrada = base_ind_04_sucia.loc[base_ind_04_sucia['aglomerado'].isin(['Ciudad de Buenos Aires', 'Partidos del GBA'])]\n",
    "base_ind_24_filtrada = base_ind_24_sucia.loc[base_ind_24_sucia['AGLOMERADO'].isin([32, 33])]\n",
    "\n",
    "#Para poder unir sin problema, me aseguro que als variables esten en el mismo formato\n",
    "\n",
    "base_ind_04_filtrada.columns = base_ind_04_filtrada.columns.str.lower()\n",
    "base_ind_24_filtrada.columns = base_ind_24_filtrada.columns.str.lower()\n",
    "\n",
    "# concateno las bases\n",
    "base_ind_prelimpieza = pd.concat([base_ind_24_filtrada, base_ind_04_filtrada])\n",
    "print(base_ind_prelimpieza.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "642e442d-0b8f-4fdf-8f0b-9d7606b0c2dd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codusu', 'ano4', 'trimestre', 'nro_hogar', 'realizada', 'region', 'mas_500', 'aglomerado', 'pondera', 'iv1', 'iv1_esp', 'iv2', 'iv3', 'iv3_esp', 'iv4', 'iv5', 'iv6', 'iv7', 'iv7_esp', 'iv8', 'iv9', 'iv10', 'iv11', 'iv12_1', 'iv12_2', 'iv12_3', 'ii1', 'ii2', 'ii3', 'ii3_1', 'ii4_1', 'ii4_2', 'ii4_3', 'ii5', 'ii5_1', 'ii6', 'ii6_1', 'ii7', 'ii7_esp', 'ii8', 'ii8_esp', 'ii9', 'v1', 'v2', 'v21', 'v22', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19_a', 'v19_b', 'ix_tot', 'ix_men10', 'ix_mayeq10', 'itf', 'decifr', 'idecifr', 'rdecifr', 'gdecifr', 'pdecifr', 'adecifr', 'ipcf', 'deccfr', 'ideccfr', 'rdeccfr', 'gdeccfr', 'pdeccfr', 'adeccfr', 'pondih', 'vii1_1', 'vii1_2', 'vii2_1', 'vii2_2', 'vii2_3', 'vii2_4', 'idimph']\n"
     ]
    }
   ],
   "source": [
    "# LIMPIO BASE HOGAR\n",
    "\n",
    "# me quedo solo con los valores de CABA y GBA\n",
    "base_hog_04_filtrada = base_hog_04_sucia.loc[base_hog_04_sucia['aglomerado'].isin(['Ciudad de Buenos Aires', 'Partidos del GBA'])]\n",
    "base_hog_24_filtrada = base_hog_24_sucia.loc[base_hog_24_sucia['AGLOMERADO'].isin([32, 33])]\n",
    "\n",
    "#Para poder unir sin problema, me aseguro que als variables esten en el mismo formato\n",
    "base_hog_04_filtrada.columns = base_hog_04_filtrada.columns.str.lower()\n",
    "base_hog_24_filtrada.columns = base_hog_24_filtrada.columns.str.lower()\n",
    "\n",
    "# concateno las bases\n",
    "base_hog_prelimpieza = pd.concat([base_hog_24_filtrada, base_hog_04_filtrada])\n",
    "print(base_hog_prelimpieza.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "10d38a55",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codusu', 'ano4_x', 'trimestre_x', 'nro_hogar', 'componente', 'h15', 'region_x', 'mas_500_x', 'aglomerado_x', 'pondera_x', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch15_cod', 'ch16', 'ch16_cod', 'nivel_ed', 'estado', 'cat_ocup', 'cat_inac', 'imputa', 'pp02c1', 'pp02c2', 'pp02c3', 'pp02c4', 'pp02c5', 'pp02c6', 'pp02c7', 'pp02c8', 'pp02e', 'pp02h', 'pp02i', 'pp03c', 'pp03d', 'pp3e_tot', 'pp3f_tot', 'pp03g', 'pp03h', 'pp03i', 'pp03j', 'intensi', 'pp04a', 'pp04b_cod', 'pp04b1', 'pp04b2', 'pp04b3_mes', 'pp04b3_ano', 'pp04b3_dia', 'pp04c', 'pp04c99', 'pp04d_cod', 'pp04g', 'pp05b2_mes', 'pp05b2_ano', 'pp05b2_dia', 'pp05c_1', 'pp05c_2', 'pp05c_3', 'pp05e', 'pp05f', 'pp05h', 'pp06a', 'pp06c', 'pp06d', 'pp06e', 'pp06h', 'pp07a', 'pp07c', 'pp07d', 'pp07e', 'pp07f1', 'pp07f2', 'pp07f3', 'pp07f4', 'pp07f5', 'pp07g1', 'pp07g2', 'pp07g3', 'pp07g4', 'pp07g_59', 'pp07h', 'pp07i', 'pp07j', 'pp07k', 'pp08d1', 'pp08d4', 'pp08f1', 'pp08f2', 'pp08j1', 'pp08j2', 'pp08j3', 'pp09a', 'pp09a_esp', 'pp09b', 'pp09c', 'pp09c_esp', 'pp10a', 'pp10c', 'pp10d', 'pp10e', 'pp11a', 'pp11b_cod', 'pp11b1', 'pp11b2_mes', 'pp11b2_ano', 'pp11b2_dia', 'pp11c', 'pp11c99', 'pp11d_cod', 'pp11g_ano', 'pp11g_mes', 'pp11g_dia', 'pp11l', 'pp11l1', 'pp11m', 'pp11n', 'pp11o', 'pp11p', 'pp11q', 'pp11r', 'pp11s', 'pp11t', 'p21', 'decocur', 'idecocur', 'rdecocur', 'gdecocur', 'pdecocur', 'adecocur', 'pondiio', 'tot_p12', 'p47t', 'decindr', 'idecindr', 'rdecindr', 'gdecindr', 'pdecindr', 'adecindr', 'pondii', 'v2_m', 'v3_m', 'v4_m', 'v5_m', 'v8_m', 'v9_m', 'v10_m', 'v11_m', 'v12_m', 'v18_m', 'v19_am', 'v21_m', 't_vi', 'itf_x', 'decifr_x', 'idecifr_x', 'rdecifr_x', 'gdecifr_x', 'pdecifr_x', 'adecifr_x', 'ipcf_x', 'deccfr_x', 'ideccfr_x', 'rdeccfr_x', 'gdeccfr_x', 'pdeccfr_x', 'adeccfr_x', 'pondih_x', 'pj1_1', 'pj2_1', 'pj3_1', 'idimpp', 'ano4_y', 'trimestre_y', 'realizada', 'region_y', 'mas_500_y', 'aglomerado_y', 'pondera_y', 'iv1', 'iv1_esp', 'iv2', 'iv3', 'iv3_esp', 'iv4', 'iv5', 'iv6', 'iv7', 'iv7_esp', 'iv8', 'iv9', 'iv10', 'iv11', 'iv12_1', 'iv12_2', 'iv12_3', 'ii1', 'ii2', 'ii3', 'ii3_1', 'ii4_1', 'ii4_2', 'ii4_3', 'ii5', 'ii5_1', 'ii6', 'ii6_1', 'ii7', 'ii7_esp', 'ii8', 'ii8_esp', 'ii9', 'v1', 'v2', 'v21', 'v22', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19_a', 'v19_b', 'ix_tot', 'ix_men10', 'ix_mayeq10', 'itf_y', 'decifr_y', 'idecifr_y', 'rdecifr_y', 'gdecifr_y', 'pdecifr_y', 'adecifr_y', 'ipcf_y', 'deccfr_y', 'ideccfr_y', 'rdeccfr_y', 'gdeccfr_y', 'pdeccfr_y', 'adeccfr_y', 'pondih_y', 'vii1_1', 'vii1_2', 'vii2_1', 'vii2_2', 'vii2_3', 'vii2_4', 'idimph']\n"
     ]
    }
   ],
   "source": [
    "#Mergeo las dos bases\n",
    "\n",
    "base_prelimpieza = pd.merge(base_ind_prelimpieza, base_hog_prelimpieza, on=[\"codusu\", \"nro_hogar\"], how=\"inner\")\n",
    "print(base_prelimpieza.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad35069f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Variable  Diferencias\n",
      "0      adeccfr            0\n",
      "1      adecifr            0\n",
      "2   aglomerado            0\n",
      "3         ano4            0\n",
      "4       deccfr            0\n",
      "5       decifr            0\n",
      "6      gdeccfr            0\n",
      "7      gdecifr            0\n",
      "8      ideccfr         7051\n",
      "9      idecifr         7051\n",
      "10        ipcf          334\n",
      "11         itf            0\n",
      "12     mas_500            0\n",
      "13     pdeccfr         7051\n",
      "14     pdecifr         7051\n",
      "15     pondera            0\n",
      "16      pondih         7647\n",
      "17     rdeccfr            0\n",
      "18     rdecifr            0\n",
      "19      region            0\n",
      "20   trimestre            0\n"
     ]
    }
   ],
   "source": [
    "# me fijo que las que aparecen en ambas bases no tengan valores diferentes \n",
    "variables_comunes = [\n",
    "    'adeccfr', 'adecifr', 'aglomerado', 'ano4', 'codusu', 'deccfr', \n",
    "    'decifr', 'gdeccfr', 'gdecifr', 'ideccfr', 'idecifr', 'ipcf', \n",
    "    'itf', 'mas_500', 'nro_hogar', 'pdeccfr', 'pdecifr', 'pondera', \n",
    "    'pondih', 'rdeccfr', 'rdecifr', 'region', 'trimestre'\n",
    "]\n",
    "\n",
    "# Crear un diccionario para almacenar los resultados\n",
    "diferencias_resumen = {}\n",
    "\n",
    "# Analizar cada variable\n",
    "for var in variables_comunes:\n",
    "    columna_x = f\"{var}_x\"\n",
    "    columna_y = f\"{var}_y\"\n",
    "    \n",
    "    # Comparar las columnas (asegúrate de que existen en la base)\n",
    "    if columna_x in base_prelimpieza.columns and columna_y in base_prelimpieza.columns:\n",
    "        # Identificar diferencias\n",
    "        diferencias = base_prelimpieza[columna_x] != base_prelimpieza[columna_y]\n",
    "        diferencias_count = diferencias.sum()\n",
    "        \n",
    "        # Almacenar resultados en el diccionario\n",
    "        diferencias_resumen[var] = diferencias_count\n",
    "\n",
    "# Convertir el resumen a un DataFrame para una mejor visualización\n",
    "import pandas as pd\n",
    "resumen_df = pd.DataFrame.from_dict(diferencias_resumen, orient=\"index\", columns=[\"Diferencias\"])\n",
    "resumen_df.index.name = \"Variable\"\n",
    "resumen_df.reset_index(inplace=True)\n",
    "\n",
    "# Mostrar las diferencias\n",
    "print(resumen_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e79fa8f1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['codusu', 'ano4', 'trimestre', 'nro_hogar', 'componente', 'h15', 'region', 'mas_500', 'aglomerado', 'pondera', 'ch03', 'ch04', 'ch05', 'ch06', 'ch07', 'ch08', 'ch09', 'ch10', 'ch11', 'ch12', 'ch13', 'ch14', 'ch15', 'ch15_cod', 'ch16', 'ch16_cod', 'nivel_ed', 'estado', 'cat_ocup', 'cat_inac', 'imputa', 'pp02c1', 'pp02c2', 'pp02c3', 'pp02c4', 'pp02c5', 'pp02c6', 'pp02c7', 'pp02c8', 'pp02e', 'pp02h', 'pp02i', 'pp03c', 'pp03d', 'pp3e_tot', 'pp3f_tot', 'pp03g', 'pp03h', 'pp03i', 'pp03j', 'intensi', 'pp04a', 'pp04b_cod', 'pp04b1', 'pp04b2', 'pp04b3_mes', 'pp04b3_ano', 'pp04b3_dia', 'pp04c', 'pp04c99', 'pp04d_cod', 'pp04g', 'pp05b2_mes', 'pp05b2_ano', 'pp05b2_dia', 'pp05c_1', 'pp05c_2', 'pp05c_3', 'pp05e', 'pp05f', 'pp05h', 'pp06a', 'pp06c', 'pp06d', 'pp06e', 'pp06h', 'pp07a', 'pp07c', 'pp07d', 'pp07e', 'pp07f1', 'pp07f2', 'pp07f3', 'pp07f4', 'pp07f5', 'pp07g1', 'pp07g2', 'pp07g3', 'pp07g4', 'pp07g_59', 'pp07h', 'pp07i', 'pp07j', 'pp07k', 'pp08d1', 'pp08d4', 'pp08f1', 'pp08f2', 'pp08j1', 'pp08j2', 'pp08j3', 'pp09a', 'pp09a_esp', 'pp09b', 'pp09c', 'pp09c_esp', 'pp10a', 'pp10c', 'pp10d', 'pp10e', 'pp11a', 'pp11b_cod', 'pp11b1', 'pp11b2_mes', 'pp11b2_ano', 'pp11b2_dia', 'pp11c', 'pp11c99', 'pp11d_cod', 'pp11g_ano', 'pp11g_mes', 'pp11g_dia', 'pp11l', 'pp11l1', 'pp11m', 'pp11n', 'pp11o', 'pp11p', 'pp11q', 'pp11r', 'pp11s', 'pp11t', 'p21', 'decocur', 'idecocur', 'rdecocur', 'gdecocur', 'pdecocur', 'adecocur', 'pondiio', 'tot_p12', 'p47t', 'decindr', 'idecindr', 'rdecindr', 'gdecindr', 'pdecindr', 'adecindr', 'pondii', 'v2_m', 'v3_m', 'v4_m', 'v5_m', 'v8_m', 'v9_m', 'v10_m', 'v11_m', 'v12_m', 'v18_m', 'v19_am', 'v21_m', 't_vi', 'itf', 'decifr', 'idecifr', 'rdecifr', 'gdecifr', 'pdecifr', 'adecifr', 'ipcf', 'deccfr', 'ideccfr', 'rdeccfr', 'gdeccfr', 'pdeccfr', 'adeccfr', 'pondih', 'pj1_1', 'pj2_1', 'pj3_1', 'idimpp', 'realizada', 'iv1', 'iv1_esp', 'iv2', 'iv3', 'iv3_esp', 'iv4', 'iv5', 'iv6', 'iv7', 'iv7_esp', 'iv8', 'iv9', 'iv10', 'iv11', 'iv12_1', 'iv12_2', 'iv12_3', 'ii1', 'ii2', 'ii3', 'ii3_1', 'ii4_1', 'ii4_2', 'ii4_3', 'ii5', 'ii5_1', 'ii6', 'ii6_1', 'ii7', 'ii7_esp', 'ii8', 'ii8_esp', 'ii9', 'v1', 'v2', 'v21', 'v22', 'v3', 'v4', 'v5', 'v6', 'v7', 'v8', 'v9', 'v10', 'v11', 'v12', 'v13', 'v14', 'v15', 'v16', 'v17', 'v18', 'v19_a', 'v19_b', 'ix_tot', 'ix_men10', 'ix_mayeq10', 'ipcf_dup', 'vii1_1', 'vii1_2', 'vii2_1', 'vii2_2', 'vii2_3', 'vii2_4', 'idimph']\n"
     ]
    }
   ],
   "source": [
    "# Mergeo las dos bases bien, fijandome que no duplique las variables \n",
    "base_prelimpieza = pd.merge(base_ind_prelimpieza, base_hog_prelimpieza, \n",
    "                          on=[\"codusu\", \"nro_hogar\"], \n",
    "                          suffixes=('', '_dup'))\n",
    "\n",
    "# Eliminar columnas duplicadas con sufijo '_dup'\n",
    "for col in base_prelimpieza.columns:\n",
    "    if col.endswith('_dup') and base_prelimpieza[col[:-4]].equals(base_prelimpieza[col]):\n",
    "        base_prelimpieza.drop(columns=col, inplace=True)\n",
    "\n",
    "print(base_prelimpieza.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9c3612d9-f083-4443-9ae1-f52f0999fbdf",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14698, 248)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_prelimpieza.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45837c45-19c6-451f-a706-c10980353a66",
   "metadata": {},
   "source": [
    "### PUNTO 3\n",
    "Limpien la base de datos tomando criterios que hagan sentido. Explicar cualquier decisión como el tratamiento de valores faltantes (missing values), extremos (outliers), o variables categóricas. Justifique sus decisiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c1e3cbb-f911-4460-b2ff-049f02cf8621",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " La cantidad de datos en la variable ch04 por valor de etiqueta \n",
      " ch04\n",
      "1    6973\n",
      "2    7725\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable ch06 por valor de etiqueta \n",
      " ch06\n",
      "-1      51\n",
      " 0     131\n",
      " 1     180\n",
      " 2     198\n",
      " 3     191\n",
      "      ... \n",
      " 94      4\n",
      " 95      2\n",
      " 96      4\n",
      " 97      1\n",
      " 98      4\n",
      "Length: 100, dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable ch07 por valor de etiqueta \n",
      " ch07\n",
      "1    2118\n",
      "2    3879\n",
      "3     797\n",
      "4     829\n",
      "5    7062\n",
      "9      13\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable ch08 por valor de etiqueta \n",
      " ch08\n",
      "1     7876\n",
      "2     1176\n",
      "3       49\n",
      "4     5321\n",
      "9       36\n",
      "12     232\n",
      "13       3\n",
      "23       5\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable nivel_ed por valor de etiqueta \n",
      " nivel_ed\n",
      "1    2273\n",
      "2    2374\n",
      "3    2790\n",
      "4    2696\n",
      "5    1583\n",
      "6    1755\n",
      "7    1227\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable estado por valor de etiqueta \n",
      " estado\n",
      "0      51\n",
      "1    6303\n",
      "2     839\n",
      "3    5462\n",
      "4    2043\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable cat_inac por valor de etiqueta \n",
      " cat_inac\n",
      "0    7193\n",
      "1    1385\n",
      "2      32\n",
      "3    3034\n",
      "4    1469\n",
      "5    1165\n",
      "6     100\n",
      "7     320\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable componente por valor de etiqueta \n",
      " componente\n",
      "1     4828\n",
      "2     3851\n",
      "3     2602\n",
      "4     1707\n",
      "5      844\n",
      "6      397\n",
      "7      214\n",
      "8      111\n",
      "9       60\n",
      "10      32\n",
      "11      17\n",
      "12      13\n",
      "13       7\n",
      "14       2\n",
      "15       2\n",
      "51      11\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable h15 por valor de etiqueta \n",
      " h15\n",
      "0     2094\n",
      "1    12604\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable mas_500 por valor de etiqueta \n",
      " mas_500\n",
      "0    14698\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable aglomerado por valor de etiqueta \n",
      " aglomerado\n",
      "32     3258\n",
      "33    11440\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable ch03 por valor de etiqueta \n",
      " ch03\n",
      "1     4896\n",
      "2     2801\n",
      "3     5614\n",
      "4      144\n",
      "5      562\n",
      "6      180\n",
      "7       57\n",
      "8      174\n",
      "9      196\n",
      "10      74\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v5 por valor de etiqueta \n",
      " v5\n",
      "1     1792\n",
      "2    12876\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v6 por valor de etiqueta \n",
      " v6\n",
      "0        7\n",
      "1     1382\n",
      "2    13279\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v7 por valor de etiqueta \n",
      " v7\n",
      "1     1084\n",
      "2    13584\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v8 por valor de etiqueta \n",
      " v8\n",
      "1      429\n",
      "2    14239\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v11 por valor de etiqueta \n",
      " v11\n",
      "1      356\n",
      "2    14312\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v12 por valor de etiqueta \n",
      " v12\n",
      "1     1182\n",
      "2    13486\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v13 por valor de etiqueta \n",
      " v13\n",
      "1     4099\n",
      "2    10566\n",
      "9       33\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v17 por valor de etiqueta \n",
      " v17\n",
      "1     1307\n",
      "2    13361\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v19_a por valor de etiqueta \n",
      " v19_a\n",
      "1       11\n",
      "2    14657\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable v19_b por valor de etiqueta \n",
      " v19_b\n",
      "1       10\n",
      "2    14658\n",
      "9       30\n",
      "dtype: int64\n",
      "\n",
      " La cantidad de datos en la variable pp02h por valor de etiqueta \n",
      " pp02h\n",
      "0    9250\n",
      "1     153\n",
      "2    5295\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['ch04'] = base_prelimpieza['ch04'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:12: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['ch06'] = base_prelimpieza['ch06'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:18: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['ch07'] = base_prelimpieza['ch07'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:25: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['ch08'] = base_prelimpieza['ch08'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:36: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['nivel_ed'] = base_prelimpieza['nivel_ed'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:46: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['estado'] = base_prelimpieza['estado'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:53: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['cat_inac'] = base_prelimpieza['cat_inac'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:63: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['componente'] = base_prelimpieza['componente'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:67: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['h15'] = base_prelimpieza['h15'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:73: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['mas_500'] = base_prelimpieza['mas_500'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:77: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['aglomerado'] = base_prelimpieza['aglomerado'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:81: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['ch03'] = base_prelimpieza['ch03'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:93: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v5'] = base_prelimpieza['v5'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:98: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v6'] = base_prelimpieza['v6'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:104: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v7'] = base_prelimpieza['v7'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:109: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v8'] = base_prelimpieza['v8'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:114: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v11'] = base_prelimpieza['v11'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:119: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v12'] = base_prelimpieza['v12'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:124: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v13'] = base_prelimpieza['v13'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:129: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v17'] = base_prelimpieza['v17'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:134: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v19_a'] = base_prelimpieza['v19_a'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:139: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['v19_b'] = base_prelimpieza['v19_b'].replace({\n",
      "/var/folders/9r/73xgxgj17t7btpfss2pdczcr0000gn/T/ipykernel_9871/795321814.py:144: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  base_prelimpieza['pp02h'] = base_prelimpieza['pp02h'].replace({\n"
     ]
    }
   ],
   "source": [
    "# TRATAMIENTO CATEGORICAS\n",
    "\n",
    "# como hay datos que tienen distintas etiquetas en las dos bases que concatenamos, tenemos que  renombrar las etiquetas de una de las bases asi se pueden tener todos los datos con el mismo valor\n",
    "variables_interes = base_prelimpieza[[\"ch04\", \"ch06\", \"ch07\", \"ch08\", \"nivel_ed\", \"estado\", \"cat_inac\", \"componente\", \"h15\", \"mas_500\", \"aglomerado\", \"ch03\", \"v5\", \"v6\", \"v7\", \"v8\", \"v11\", \"v12\", \"v13\", \"v17\", \"v19_a\", \"v19_b\", \"pp02h\"]]\n",
    "\n",
    "for i in variables_interes:\n",
    "    if i == \"ch04\": \n",
    "        base_prelimpieza['ch04'] = base_prelimpieza['ch04'].replace({\n",
    "            'Mujer': 2,\n",
    "            'Varón': 1})\n",
    "    elif i == \"ch06\":\n",
    "        base_prelimpieza['ch06'] = base_prelimpieza['ch06'].replace({\n",
    "            '98 y más años' : 98, \n",
    "            'Menos de 1 año' : 0})\n",
    "        # cambio el formato de los valores de esta columna porque la mitad estaba en float y la mitas en int\n",
    "        base_prelimpieza['ch06'] = base_prelimpieza['ch06'].astype(int)\n",
    "    elif i == \"ch07\":\n",
    "        base_prelimpieza['ch07'] = base_prelimpieza['ch07'].replace({\n",
    "            'Unido': 1, \n",
    "            'Casado': 2, \n",
    "            'Separado o divorciado': 3, \n",
    "            'Viudo': 4, \n",
    "            'Soltero': 5})\n",
    "    elif i == \"ch08\":\n",
    "        base_prelimpieza['ch08'] = base_prelimpieza['ch08'].replace({\n",
    "            'Obra social (incluye PAMI)': 1,  \n",
    "            'Mutual/Prepaga/Servicio de emergencia': 2,  \n",
    "            'Planes y seguros públicos': 3,  \n",
    "            'No paga ni le descuentan': 4,  \n",
    "            'Ns./Nr.': 9,  \n",
    "            'Obra social y mutual/prepaga/servicio de emergencia': 12,  \n",
    "            'Obra social y planes y seguros públicos': 13,  \n",
    "            'Mutual/prepaga/servicio de emergencia/planes y seguros públi': 23,\n",
    "            'Obra Social, mutual/prepaga/servicio de emergencia y planes y seguros públicos': 123})\n",
    "    elif i == \"nivel_ed\":\n",
    "        base_prelimpieza['nivel_ed'] = base_prelimpieza['nivel_ed'].replace({\n",
    "            'Primaria Incompleta (incluye educación especial)': 1,  \n",
    "            'Primaria Completa': 2,  \n",
    "            'Secundaria Incompleta': 3,  \n",
    "            'Secundaria Completa': 4,  \n",
    "            'Superior Universitaria Incompleta': 5,  \n",
    "            'Superior Universitaria Completa': 6, \n",
    "            'Sin instrucción': 7,  \n",
    "            'Ns./Nr.': 9})\n",
    "    elif i == \"estado\":\n",
    "        base_prelimpieza['estado'] = base_prelimpieza['estado'].replace({\n",
    "            'Ocupado': 1,  \n",
    "            'Desocupado': 2,  \n",
    "            'Inactivo': 3,  \n",
    "            'Menor de 10 años': 4,  \n",
    "            'Entrevista individual no realizada (no respuesta al cuestion': 0})\n",
    "    elif i == \"cat_inac\":\n",
    "        base_prelimpieza['cat_inac'] = base_prelimpieza['cat_inac'].replace({\n",
    "            0.0 : 0,\n",
    "            'Jubilado/pensionado': 1,  \n",
    "            'Rentista': 2,  \n",
    "            'Estudiante': 3,  \n",
    "            'Ama de casa': 4,  \n",
    "            'Menor de 6 años': 5,  \n",
    "            'Discapacitado': 6,  \n",
    "            'Otros': 7})\n",
    "    elif i == \"componente\":\n",
    "        base_prelimpieza['componente'] = base_prelimpieza['componente'].replace({\n",
    "            'Servicio doméstico en hogares': 51})\n",
    "        base_prelimpieza['componente'] = base_prelimpieza['componente'].astype(int)\n",
    "    elif i == \"h15\":\n",
    "        base_prelimpieza['h15'] = base_prelimpieza['h15'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 0,\n",
    "            2 : 0})\n",
    "        base_prelimpieza['h15'] = base_prelimpieza['h15'].astype(int)\n",
    "    elif i == \"mas_500\":\n",
    "        base_prelimpieza['mas_500'] = base_prelimpieza['mas_500'].replace({\n",
    "            'N' : 1,\n",
    "            'S' : 0})\n",
    "    elif i == \"aglomerado\":\n",
    "        base_prelimpieza['aglomerado'] = base_prelimpieza['aglomerado'].replace({\n",
    "            'Ciudad de Buenos Aires' : 32,\n",
    "            'Partidos del GBA' : 33})\n",
    "    elif i == \"ch03\":\n",
    "        base_prelimpieza['ch03'] = base_prelimpieza['ch03'].replace({\n",
    "            'Jefe' : 1,\n",
    "            'Cónyuge/Pareja' : 2,\n",
    "            'Hijo/Hijastro' : 3,\n",
    "            'Yerno/Nuera' : 4,\n",
    "            'Nieto' : 5,\n",
    "            'Madre/Padre' : 6,\n",
    "            'Suegro' : 7,\n",
    "            'Hermano' : 8,\n",
    "            'Otros familiares' : 9,\n",
    "            'No familiares' : 10})\n",
    "    elif i == \"v5\":\n",
    "        base_prelimpieza['v5'] = base_prelimpieza['v5'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"v6\":\n",
    "        base_prelimpieza['v6'] = base_prelimpieza['v6'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})    \n",
    "        base_prelimpieza['v6'] = base_prelimpieza['v6'].astype(int)\n",
    "    elif i == \"v7\":\n",
    "        base_prelimpieza['v7'] = base_prelimpieza['v7'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"v8\":\n",
    "        base_prelimpieza['v8'] = base_prelimpieza['v8'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"v11\":\n",
    "        base_prelimpieza['v11'] = base_prelimpieza['v11'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"v12\":\n",
    "        base_prelimpieza['v12'] = base_prelimpieza['v12'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"v13\":\n",
    "        base_prelimpieza['v13'] = base_prelimpieza['v13'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"v17\":\n",
    "        base_prelimpieza['v17'] = base_prelimpieza['v17'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})    \n",
    "    elif i == \"v19_a\":\n",
    "        base_prelimpieza['v19_a'] = base_prelimpieza['v19_a'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"v19_b\":\n",
    "        base_prelimpieza['v19_b'] = base_prelimpieza['v19_b'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "    elif i == \"pp02h\":\n",
    "        base_prelimpieza['pp02h'] = base_prelimpieza['pp02h'].replace({\n",
    "            \"Sí\": 1,\n",
    "            \"No\": 2,\n",
    "            \"Ns./Nr.\":9})\n",
    "        base_prelimpieza['pp02h'] = base_prelimpieza['pp02h'].astype(int)\n",
    "\n",
    "    conteo = base_prelimpieza.groupby(i).size()\n",
    "    print(\"\\n\", \"La cantidad de datos en la variable\", i, \"por valor de etiqueta\", \"\\n\", conteo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d7c60-5047-4f9c-877b-a16d65cf3eb8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# VALORES NEGATIVOS EN EDAD\n",
    "\n",
    "valores_negativos_edad = base_prelimpieza['ch06'] < 0\n",
    "# Cantidad de datos negativos\n",
    "cantidad_negativos_edad = valores_negativos_edad.sum()\n",
    "print(\"Cantidad de valores negativos en variable edad:\", cantidad_negativos_edad)\n",
    "\n",
    "# Me quedo con los valores mayores o iguales a 0 de edad\n",
    "base_limpia = base_prelimpieza[base_prelimpieza['ch06'] >= 0]\n",
    "valores_negativos_edad_post = base_limpia['ch06'] < 0\n",
    "cantidad_negativos_edad_post = valores_negativos_edad_post.sum()\n",
    "print(\"Cantidad de valores negativos en variable edad post limpieza :\", cantidad_negativos_edad_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00739257-4a30-474a-84ad-015ced00cfb1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# VALORES NEGATIVOS EN INGRESO POR CIRCULO FAMILIAR\n",
    "\n",
    "valores_negativos_ipcf = base_prelimpieza[base_prelimpieza['ipcf'] < 0]\n",
    "# Cantidad de datos negativos\n",
    "cantidad_negativos_ipcf = len(valores_negativos_ipcf)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(\"Cantidad de valores negativos en variable ipcf:\", cantidad_negativos_ipcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b6f736",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#  MISSING VALUES\n",
    "\n",
    "missing_values = variables_interes.isnull().sum()\n",
    "\n",
    "# Mostrar las columnas con el número de valores faltantes\n",
    "print(missing_values)\n",
    "\n",
    "# Mostrar solo las columnas con valores faltantes\n",
    "missing_values_with_data = missing_values[missing_values > 0]\n",
    "print(\"\\nColumnas con valores faltantes:\")\n",
    "print(missing_values_with_data)\n",
    "\n",
    "# En las variables de hogar habia la misma cantidad en todas que ponian no sabe no responde, por lo que quiero confirmar que son las mismas personas asi solo clasifico esos valores como missing values\n",
    "columnas = ['v5', 'v6', 'v7', 'v8', 'v11', 'v12', 'v13', 'v17', 'v19_a', 'v19_b']\n",
    "# Filtramos las filas donde las columnas seleccionadas tienen el valor \"9\"\n",
    "mask = (base_prelimpieza[columnas] == 9).all(axis=1)\n",
    "# Obtenemos solo las filas de las columnas que nos interesan\n",
    "filas_ns_nr = base_prelimpieza.loc[mask, columnas]\n",
    "# Mostrar las filas donde todas las columnas tienen el valor \"9\"\n",
    "print(\"ni idea\", filas_ns_nr)\n",
    "\n",
    "# Como vi que si son, reemplazo los valores \"9\" por NaN en las columnas seleccionadas\n",
    "base_prelimpieza[columnas] = base_prelimpieza[columnas].replace(9, pd.NA)\n",
    "# Contar cuántos NaN hay en las columnas seleccionadas\n",
    "missing_count = base_prelimpieza[columnas].isna().sum()\n",
    "# Mostrar el conteo de valores faltantes (NaN)\n",
    "print(\"la cantidad de NA es: \", missing_count)\n",
    "\n",
    "# Eliminar filas con NaN en las columnas seleccionadas\n",
    "base_limpia = base_prelimpieza.dropna(subset = columnas)\n",
    "\n",
    "# Ver el DataFrame resultante\n",
    "base_limpia.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e31e16",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# CATEGORICAS A DUMMIES\n",
    "\n",
    "base_limpia[[\"ch04\", \"ch07\", \"ch08\", \"nivel_ed\", \"estado\", \"cat_inac\", \"componente\", \"h15\", \"mas_500\", \"aglomerado\", \"ch03\", \"v5\", \"v6\", \"v7\", \"v8\", \"v11\", \"v12\", \"v13\", \"v17\", \"v19_a\", \"v19_b\", \"pp02h\"]] = base_prelimpieza[[\"ch04\", \"ch07\", \"ch08\", \"nivel_ed\", \"estado\", \"cat_inac\", \"componente\", \"h15\", \"mas_500\", \"aglomerado\", \"ch03\", \"v5\", \"v6\", \"v7\", \"v8\", \"v11\", \"v12\", \"v13\", \"v17\", \"v19_a\", \"v19_b\", \"pp02h\"]].astype('category')\n",
    "base_limpia_dummies = pd.get_dummies(base_limpia, columns=[\"ch04\", \"ch07\", \"ch08\", \"nivel_ed\", \"estado\", \"cat_inac\", \"componente\", \"h15\", \"mas_500\", \"aglomerado\", \"ch03\", \"v5\", \"v6\", \"v7\", \"v8\", \"v11\", \"v12\", \"v13\", \"v17\", \"v19_a\", \"v19_b\", \"pp02h\"])\n",
    "\n",
    "print(base_limpia_dummies.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbb8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables de interés\n",
    "variables_interes = [\"ano4\", \"ch04\", \"ch06\", \"ch07\", \"ch08\", \"nivel_ed\", \"estado\", \n",
    "                     \"cat_inac\", \"componente\", \"h15\", \"mas_500\", \"aglomerado\", \n",
    "                     \"ch03\", \"v5\", \"v6\", \"v7\", \"v8\", \"v11\", \"v12\", \"v13\", \"v17\", \n",
    "                     \"v19_a\", \"v19_b\", \"pp02h\"]\n",
    "\n",
    "# Filtrar la base para que solo contenga las variables de interés\n",
    "base_limpia_interes = base_prelimpieza[variables_interes]\n",
    "\n",
    "# Listado de columnas que queremos convertir a dummies\n",
    "columns_to_dummy = [\"ch04\", \"ch07\", \"ch08\", \"nivel_ed\", \"estado\", \"cat_inac\", \n",
    "                    \"h15\", \"mas_500\", \"aglomerado\", \"ch03\", \"v5\", \"v6\", \"v7\", \"v8\", \n",
    "                    \"v11\", \"v12\", \"v13\", \"v17\", \"v19_a\", \"v19_b\", \"pp02h\"]\n",
    "\n",
    "# Aplicar pd.get_dummies() solo a las columnas que queremos convertir a dummies\n",
    "base_limpia_dummies = pd.get_dummies(base_limpia_interes, columns=columns_to_dummy)\n",
    "\n",
    "# Convertir los valores True/False a 0/1\n",
    "base_limpia_dummies = base_limpia_dummies.astype(int)\n",
    "\n",
    "# Ver las primeras filas para confirmar que se mantuvieron las variables originales y se crearon las dummies\n",
    "print(base_limpia_dummies.head())\n",
    "\n",
    "# Ver las columnas de la base resultante\n",
    "print(base_limpia_dummies.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d10fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumar todos los valores de la columna 'h15_2'\n",
    "suma_h15_2 = base_limpia_dummies['h15_1'].sum()\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(suma_h15_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34177a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h15\n",
      "1    12588\n",
      "0     2050\n",
      "2       27\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(base_limpia['h15'].value_counts())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed986741-ddad-4355-aefb-91be96d8198e",
   "metadata": {},
   "source": [
    "# OUTLIERS DEL TP2\n",
    "\n",
    "### DENSIDAD DE PRECIOS DE ALOJAMIENTOS --> VER OUTLIERS\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.histplot(base_airbnb_limpia['price'], bins = 30, color = '#3685B5', kde = False, stat = \"density\", alpha = 0.6)\n",
    "sns.kdeplot(base_airbnb_limpia['price'], color = '#EE6C4D', linewidth = 2)\n",
    "plt.title('Densidad de precios de alojamientos')\n",
    "plt.xlabel('Precio')\n",
    "plt.ylabel('Densidad')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# OUTLIERS DE PRECIO POR TIPO DE HABITACION POR LOCALIDAD\n",
    "Q1_1 = base_airbnb_limpia.groupby(['neighbourhood_group', 'room_type'])['price'].quantile(0.25)\n",
    "Q3_1 = base_airbnb_limpia.groupby(['neighbourhood_group', 'room_type'])['price'].quantile(0.75)\n",
    "IQR_1 = Q3_1 - Q1_1\n",
    "lower_precio_hab_loc = Q1_1 - 1.5 * IQR_1\n",
    "upper_precio_hab_loc = Q3_1 + 1.5 * IQR_1\n",
    "# Crear un DataFrame con los límites de los cuantiles\n",
    "limits_precio_hab_loc = pd.DataFrame({'lower_precio_hab_loc': lower_precio_hab_loc, 'upper_precio_hab_loc': upper_precio_hab_loc}).reset_index()\n",
    "# Unir los límites al DataFrame original en los localidad\n",
    "base_airbnb_limpia = pd.merge(base_airbnb_limpia, limits_precio_hab_loc, on = ['neighbourhood_group', 'room_type'])\n",
    "# me quedo solo con los datos uqe son mayor o igual a \"lower\" y menor o igual a \"upper\"\n",
    "base_airbnb_limpia = base_airbnb_limpia[\n",
    "    (base_airbnb_limpia['price'] >= base_airbnb_limpia['lower_precio_hab_loc']) & \n",
    "    (base_airbnb_limpia['price'] <= base_airbnb_limpia['upper_precio_hab_loc'])]\n",
    "\n",
    "# DENSIDAD DE PRECIOS DE ALOJAMIENTOS POST OUTLIERS\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.histplot(base_airbnb_limpia['price'], bins = 30, color = '#3685B5', kde = False, stat = \"density\", alpha = 0.6)\n",
    "sns.kdeplot(base_airbnb_limpia['price'], color = '#EE6C4D', linewidth = 2)\n",
    "plt.title('Densidad de precios de alojamientos post outliers')\n",
    "plt.xlabel('Precio')\n",
    "plt.ylabel('Densidad')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c891a363-a497-42c9-9719-bc889e170ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTLIERS DE ESTE TP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75173cc-ed4d-42c6-a3c8-730d33bd0883",
   "metadata": {},
   "source": [
    "### PUNTO 4\n",
    "Construya variables (mínimo 3) que no estén en la base pero que sean relevantes para predecir individuos desocupados (por ejemplo, la proporción de personas que trabajan en el hogar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79120c-7c1e-4519-b2fb-5b78e57c1b8e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PROPORCION DE PERSONAS QUE TRABAJAN EN UN HOGAR\n",
    "\n",
    "# Tomo los datos de hogares con trabajadores ocupadas (estado_1)\n",
    "base_limpia_dummies['trabajadores_hogar'] = base_limpia_dummies['estado_1']\n",
    "\n",
    "# Calcular la proporción de trabajadores en el hogar\n",
    "base_limpia_dummies['proporcion_trabajadores'] = base_limpia_dummies['trabajadores_hogar'] / base_limpia_dummies['ix_tot']\n",
    "base_limpia_dummies[['proporcion_trabajadores']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8b4f3-da78-451b-ac52-6c1ecdb57151",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PROPORCION DE INGRESOS POR AYUDAS EXTERNAS\n",
    "\n",
    "# tomo los datos de hogares que reciben ayudas externas y sumo la cantidad de ayudas que reciben\n",
    "base_limpia_dummies[\"total_ayudas\"] = base_limpia_dummies[[\"v5_1\", \"v6_1\", \"v7_1\", \"v12_1\"]].sum(axis=1)\n",
    "\n",
    "# Calcular la proporción de ayudas en el hogar\n",
    "base_limpia_dummies[\"ingreso_externo\"] = base_limpia_dummies['total_ayudas'] / base_limpia_dummies['ix_tot']\n",
    "base_limpia_dummies[['ingreso_externo']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c890da-0ce8-40a0-ae02-e37fb72dab34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# PROPORCION DE VENTA DE OBJETOS MATERIALES Y GASTO DE AHORROS EN EL HOGAR\n",
    "\n",
    "base_limpia_dummies[\"venta_pertenencias\"] = base_limpia_dummies[[\"v13_1\", \"v17_1\"]].sum(axis = 1)\n",
    "\n",
    "# Calcular la proporción de ayudas en el hogar\n",
    "base_limpia_dummies[\"ingreso_ventas\"] = base_limpia_dummies['venta_pertenencias'] / base_limpia_dummies['ix_tot']\n",
    "base_limpia_dummies[['ingreso_ventas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ce242a-ed2c-42ff-a18f-c9bfa8810e2b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# TRABAJO INFANTIL EN EL HOGAR\n",
    "\n",
    "base_limpia_dummies[\"total_infante\"] = base_limpia_dummies[[\"v19_a_1\", \"v19_b_1\"]].sum(axis = 1)\n",
    "\n",
    "# Calcular la proporción de ayudas en el hogar\n",
    "base_limpia_dummies[\"ingreso_infante\"] = base_limpia_dummies['total_infante'] / base_limpia_dummies['ix_tot']\n",
    "base_limpia_dummies[['ingreso_infante']]\n",
    "\n",
    "#veo si hay observaciones con trabajo infantil: mostrar valores distintos de 0\n",
    "valores_no_cero = base_limpia_dummies[base_limpia_dummies[\"ingreso_infante\"] != 0]\n",
    "print(valores_no_cero)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2048b3df-14e4-4c93-b677-89b012e4eda2",
   "metadata": {},
   "source": [
    "### PUNTO 5\n",
    "Presenten estadísticas descriptivas de tres variables de la encuesta de hogar que ustedes creen que pueden ser relevantes para predecir la desocupación. Comenten las estadísticas obtenidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bc8f83-39b7-4ffe-a58d-15aedad4b5ef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# V5 (si en los últimos 3 meses vivieron de subsidios o ayuda social)\n",
    "\n",
    "#x = ano4\n",
    "#y = Proporción subsidio\n",
    "#hue = V5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea70ce5-27a1-4264-970f-d34da5f20a52",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# V6 (si en los últimos 3 meses vivieron con mercaderia, ropa, alimentos del gobierno, iglesias, etc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c657a5f-2d7b-4972-b3bd-946155fd0ce6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# V7 (si en los últimos 3 meses vivieron con mercaderia, ropa, alimentos de personas externas al hogar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a377ecd-e43e-4a47-9ae4-283978c631ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# V13 (si en los ultimos 3 meses vivieron gastando lo que tenian ahorrado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce745a86-6f30-42b4-8d56-45d2b3f3b6a4",
   "metadata": {},
   "source": [
    "## Parte II: Clasificación y regularización\n",
    "El objetivo de esta parte del trabajo es nuevamente intentar predecir si una persona está desocupada o no. Esta vez utilizando distintas variables de características individuales y del hogar del encuestado. A su vez, incluiremos ejercicios de regularización y de validación cruzada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf56e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base con datos que sí respondieron a la condición de actividad\n",
    "respondieron = base_limpia_dummies[base_limpia_dummies['estado_0'] == 0]\n",
    "respondieron_cant = respondieron.shape[0]\n",
    "\n",
    "print(\"La cantidad de personas que respondieron a la pregunta de condición de actividad son:\", respondieron_cant)\n",
    "\n",
    "# Base con datos que no respondieron a la condición de actividad\n",
    "no_respondieron = base_limpia_dummies[base_limpia_dummies['estado_0'] == 1]\n",
    "no_respondieron_cant = no_respondieron.shape[0]\n",
    "\n",
    "print(\"La cantidad de personas que no respondieron a la pregunta de condición de actividad son:\", no_respondieron_cant)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbee8e0-e96e-4aa4-bf97-4075f747939e",
   "metadata": {},
   "source": [
    "### PUNTO 1\n",
    "Para cada año, partan la base respondieron en una base de prueba y una de entrenamiento (X_train, y_train, X_test, y_test) utilizando el comando train_test_split. La base de entrenamiento debe comprender el 70% de los datos, y la semilla a utilizar (random state instance) debe ser 101. Establezca a desocupado como su variable dependiente en la base de entrenamiento (vector y). El resto de las variables serán las variables independientes (matriz X). Recuerden agregar la columna de unos (1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e14bbf-aac7-4783-b6db-316e84a30600",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def preparar_datos(base, ano):\n",
    "    \"\"\"\n",
    "    Prepara los conjuntos de entrenamiento y prueba para un año específico.\n",
    "    \n",
    "    Args:\n",
    "        base: DataFrame original.\n",
    "        ano: Año para filtrar los datos.\n",
    "\n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test: Conjuntos de datos preparados.\n",
    "    \"\"\"\n",
    "    # Filtrar los datos por año\n",
    "    datos_ano = base[base['ano4'] == ano]\n",
    "    \n",
    "    # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "    train, test = train_test_split(datos_ano, test_size=0.3, random_state=101)\n",
    "\n",
    "    # Separar características y variable objetivo para entrenamiento\n",
    "    X_train = train.copy()  # Mantener todas las columnas en X_train\n",
    "    y_train = train['estado_2']  # Variable dependiente: estado_2\n",
    "\n",
    "    # Agregar columna de intercepto\n",
    "    X_train = X_train.assign(intercept=1)\n",
    "\n",
    "    # Separar características y variable objetivo para prueba\n",
    "    X_test = test.copy()  # Mantener todas las columnas en X_test\n",
    "    y_test = test['estado_2']  # Variable dependiente: estado_2\n",
    "\n",
    "    # Agregar columna de intercepto\n",
    "    X_test = X_test.assign(intercept=1)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Ejemplo de uso con los años disponibles\n",
    "X_train_2004, y_train_2004, X_test_2004, y_test_2004 = preparar_datos(respondieron, 2004)\n",
    "X_train_2024, y_train_2024, X_test_2024, y_test_2024 = preparar_datos(respondieron, 2024)\n",
    "\n",
    "# Imprimir el tamaño de las particiones para un año\n",
    "print(f\"Tamaño de X_train para 2004: {X_train_2004.shape}\")\n",
    "print(f\"Tamaño de y_train para 2004: {y_train_2004.shape}\")\n",
    "print(f\"Tamaño de X_test para 2024: {X_test_2024.shape}\")\n",
    "print(f\"Tamaño de y_test para 2024: {y_test_2024.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021f301-4300-465c-8d72-5cedcd2b0ea8",
   "metadata": {},
   "source": [
    "### PUNTO 2\n",
    "Expliquen brevemente cómo elegirían λ por validación cruzada (en Python es alpha). Detallen por qué no usarían el conjunto de prueba(test) para su elección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63229baf-a192-4f9e-b6a6-f04377cba752",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc38cd07-0ef9-4907-864f-0764398e94a6",
   "metadata": {},
   "source": [
    "### PUNTO 3\n",
    "En validación cruzada, ¿cuáles son las implicancias de usar un k muy pequeño o uno muy grande? Cuando k = n (con n el número de muestras), ¿cuántas veces se estima el modelo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bd1926-828b-4fa0-8775-8aa1c6d5496e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a0cac67-43fb-4924-8d5f-7b63593b6683",
   "metadata": {},
   "source": [
    "### PUNTO 4\n",
    "Para regresión logística, implementen la penalidad, L1 como la de LASSO y L2 comola de Ridge con (como en la Tutorial 10), usando λ = 1 la opción penalty y reporten la matriz de confusión, la curva ROC, los valores de AUC y de Accuracy para cada año (En la clase magistral 9, vimos el método de regularización en regresión lineal donde la variable dependiente es numérica. En este caso, nuestra variable dependiente es binaria (ocupado, desocupado), por lo que usamos la regresión logística y aprovechamos la opción de penalidad para aplicar los métodos de regularización vistos en clase.) ¿Cómo cambiaron los resultados con respecto al TP3? ¿La performance de regresión logística con regularización es mejor o peor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b42ee5-37f9-45bc-a6f0-d364e6cb0c91",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def entrenar_evaluar_logistica(X_train, y_train, X_test, y_test, penalty):\n",
    "    \"\"\"\n",
    "    Entrena un modelo de regresión logística con penalización L1 o L2 y evalúa su desempeño.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Matriz de características para entrenamiento.\n",
    "        y_train: Vector de etiquetas para entrenamiento.\n",
    "        X_test: Matriz de características para prueba.\n",
    "        y_test: Vector de etiquetas para prueba.\n",
    "        penalty: Tipo de penalización ('l1' o 'l2').\n",
    "\n",
    "    Returns:\n",
    "        result: Diccionario con métricas de evaluación (matriz de confusión, AUC, Accuracy).\n",
    "    \"\"\"\n",
    "    # Entrenar el modelo\n",
    "    modelo = LogisticRegression(penalty=penalty, C=1, solver='saga', max_iter=1000)\n",
    "    modelo.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred = modelo.predict(X_test)\n",
    "    y_prob = modelo.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calcular métricas\n",
    "    matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc_score = auc(fpr, tpr)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Graficar la curva ROC\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr, tpr, label=f\"ROC curve (AUC = {auc_score:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f\"Curva ROC - Penalización {penalty.upper()}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'Matriz de Confusión': matriz_confusion,\n",
    "        'AUC': auc_score,\n",
    "        'Accuracy': accuracy\n",
    "    }\n",
    "\n",
    "# Evaluar para 2004 y 2024 con penalización L1 y L2\n",
    "resultados_2004_L1 = entrenar_evaluar_logistica(X_train_2004, y_train_2004, X_test_2004, y_test_2004, penalty='l1')\n",
    "resultados_2004_L2 = entrenar_evaluar_logistica(X_train_2004, y_train_2004, X_test_2004, y_test_2004, penalty='l2')\n",
    "resultados_2024_L1 = entrenar_evaluar_logistica(X_train_2024, y_train_2024, X_test_2024, y_test_2024, penalty='l1')\n",
    "resultados_2024_L2 = entrenar_evaluar_logistica(X_train_2024, y_train_2024, X_test_2024, y_test_2024, penalty='l2')\n",
    "\n",
    "# Imprimir resultados\n",
    "print(\"Resultados para 2004 con penalización L1:\", resultados_2004_L1)\n",
    "print(\"Resultados para 2004 con penalización L2:\", resultados_2004_L2)\n",
    "print(\"Resultados para 2024 con penalización L1:\", resultados_2024_L1)\n",
    "print(\"Resultados para 2024 con penalización L2:\", resultados_2024_L2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10515bf-5586-4603-ad1c-a9f86f9f6c14",
   "metadata": {},
   "source": [
    "### PUNTO 5\n",
    "Realicen un barrido en = 10n con n ∈ {−5, −4, −3 ..., +4, +5} y utilicen 10-fold CV para elegir el óptimo en regresión logística con Ridge y con λ LASSO. ¿Qué seleccionó en cada caso? Usando la librería de seaborn, generen box plot mostrando la distribución del error de predicción para cada . Cada box debe corresponder a un valor de y contener como λ observaciones el error medio de validación (MSE) para cada partición. Además, para la regularización LASSO, generen un line plot del promedio de la proporción de variables ignoradas por el modelo en función de (como vieron en el tutorial 10), es decir la proporción de λ variables para las cuales el coeficiente asociado es cero (Hint: a mayor penalidad, esperamos que más coeficientes sean 0, por lo tanto, esta figura debe tener una forma de “S”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dc9cd9-b4be-4881-9308-82635b94d48e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18084909-4bde-46ce-828f-15b93858cb32",
   "metadata": {},
   "source": [
    "### PUNTO 6\n",
    "En el caso del valor óptimo de para LASSO encontrado en el inciso λ anterior, ¿qué variables fueron descartadas? ¿Son las que hubieran esperado? ¿Tiene relación con lo que respondieron en el inciso 1 de la Parte I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e1eb6-034f-40b4-b388-3526beb452e3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1266b28-4de9-45cd-8106-3f4da4cba74c",
   "metadata": {},
   "source": [
    "### PUNTO 7\n",
    "Elijan alguno de los modelos de regresión logística donde hayan probado distintos parámetros de regularización y comenten: Compare los resultados de 2004 versus 2024, ¿qué método de regularización funcionó mejor: Ridge o LASSO? ¿LASSO hizo una selección distinta de predictores en 2004 versus 2024? Comenten mencionando el error cuadrático medio (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e969f0a2-1c23-4aef-bf51-c87a0a6d31f0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
